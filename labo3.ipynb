{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "# %pip install azure-ai-ml\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## Either get environment variables, or a fallback name, which is the second parameter.\n",
    "## Currently, fill in the fallback values. Later on, we will make sure to work with Environment values. So we're already preparing for it in here!\n",
    "workspace_name = os.environ.get('WORKSPACE', 'mlops')\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID', '9dfa7b7b-77cd-4d7c-bcab-e0756bdf40a9')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP', 'mlops-labo3-tibe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we are running this in an interactive notebook; we can use the InteractiveBrowserCredential\n",
    "# This allows us to open a browser window and login there\n",
    "credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class WorkspaceHubOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "ml_client = MLClient(\n",
    "    credential, subscription_id, resource_group, workspace_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare a Virtual PC if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComputeInstance({'state': 'Running', 'last_operation': {'operation_name': 'Create', 'operation_time': '2023-10-10T12:09:41.616Z', 'operation_status': 'Succeeded', 'operation_trigger': 'User'}, 'os_image_metadata': <azure.ai.ml.entities._compute._image_metadata.ImageMetadata object at 0x000002342D29E920>, 'services': [{'display_name': 'Jupyter', 'endpoint_uri': 'https://mlopsci.westeurope.instances.azureml.ms/tree/'}, {'display_name': 'Jupyter Lab', 'endpoint_uri': 'https://mlopsci.westeurope.instances.azureml.ms/lab'}], 'type': 'computeinstance', 'created_on': None, 'provisioning_state': 'Succeeded', 'provisioning_errors': None, 'name': 'mlopsci', 'description': None, 'tags': None, 'properties': {}, 'print_as_yaml': True, 'id': '/subscriptions/9dfa7b7b-77cd-4d7c-bcab-e0756bdf40a9/resourceGroups/mlops-labo3-tibe/providers/Microsoft.MachineLearningServices/workspaces/mlops/computes/mlopsci', 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\tibed\\\\OneDrive - Hogeschool West-Vlaanderen\\\\Documenten\\\\school\\\\semester 5\\\\Mlops\\\\labo3', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000002342D29D960>, 'resource_id': None, 'location': 'westeurope', 'size': 'STANDARD_DS3_V2', 'ssh_public_access_enabled': False, 'create_on_behalf_of': None, 'network_settings': <azure.ai.ml.entities._compute.compute.NetworkSettings object at 0x000002342D29D9F0>, 'ssh_settings': <azure.ai.ml.entities._compute.compute_instance.ComputeInstanceSshSettings object at 0x000002342D29E0B0>, 'schedules': None, 'identity': None, 'idle_time_before_shutdown': None, 'idle_time_before_shutdown_minutes': None, 'setup_scripts': None, 'enable_node_public_ip': True, 'custom_applications': None, 'subnet': None})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml.entities import ComputeInstance, AmlCompute\n",
    "import datetime\n",
    "\n",
    "ci_basic_name = \"mlopsci\"\n",
    "ci_basic = ComputeInstance(name=ci_basic_name, size=\"STANDARD_DS3_v2\")\n",
    "ml_client.begin_create_or_update(ci_basic).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# STANDARD_A4M_V2\n",
    "cpu_compute_target = \"cpu-automated-test\"\n",
    "\n",
    "\n",
    "# let's see if the compute target already exists\n",
    "cpu_machine = ml_client.compute.get(cpu_compute_target)\n",
    "print(\n",
    "    f\"You already have a machine named {cpu_compute_target}, we'll reuse it as is.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name aml-Pillow is registered to workspace, the environment version is 1\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "import os\n",
    "\n",
    "custom_env_name = \"aml-Pillow\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for Image Processing (with Pillow)\",\n",
    "    tags={\"Pillow\": \"10.0.1\"},\n",
    "    conda_file=os.path.join(\"components\", \"dataprep\", \"conda.yaml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "# This registers a component with the name \"data_prep_image_resize\"\n",
    "# Which can then be used in the Pipeline editor of the Azure Portal\n",
    "data_prep_component = command(\n",
    "    name=\"data_prep_image_resize\",\n",
    "    display_name=\"Data preparation, Image Resizing\",\n",
    "    description=\"Reads a data asset of images and preprocesses them by resizing them to 64 to 64.\",\n",
    "    inputs={\n",
    "        \"data\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"output_data\": Output(type=\"uri_folder\", mode=\"rw_mount\")\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=os.path.join(\"components\", \"dataprep\"),\n",
    "    command=\"\"\"python dataprep.py \\\n",
    "            --data ${{inputs.data}} \\\n",
    "            --output_data ${{outputs.output_data}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component data_prep_image_resize with Version 2023-10-10-12-23-40-8725006 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "data_prep_component = ml_client.create_or_update(data_prep_component.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=\"mlopsci\",\n",
    "    description=\"Custom data_prep pipeline\",\n",
    ")\n",
    "def animal_images_preprocessing_pipeline(\n",
    "    input_version: str, # Currently we don't use these version numbers, but we will use them later on.\n",
    "    output_version: str,\n",
    "):\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    # These are the animals with the version name as a second item in the tuple\n",
    "    animals = [\n",
    "        ('pandas', \"1\"),\n",
    "        ('cats', \"1\"),\n",
    "        ('dogs', \"1\")\n",
    "    ] # They are hardcoded in here, because we should give them from another component otherwise.\n",
    "    \n",
    "    jobs = {}\n",
    "    for animal in animals:\n",
    "\n",
    "        data_prep_job = data_prep_component(\n",
    "            data=Input(\n",
    "                type=\"uri_folder\",\n",
    "                path=f\"azureml:{animal[0]}:{animal[1]}\" # There was a typo here that I fixed\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        output_name = animal[0] + \"_resized\"\n",
    "        output_path = \"azureml://subscriptions/9dfa7b7b-77cd-4d7c-bcab-e0756bdf40a9/resourcegroups/mlops-labo3-tibe/workspaces/mlops/datastores/workspaceblobstore/paths/processed_animals/\" + animal[0]\n",
    "\n",
    "        data_prep_job.outputs.output_data = Output(\n",
    "            type=\"uri_folder\",\n",
    "            path=output_path,\n",
    "            name=output_name,\n",
    "            mode=\"rw_mount\"\n",
    "        )\n",
    "\n",
    "        jobs[animal[0]] = data_prep_job\n",
    "\n",
    "    # a pipeline returns a dictionary of outputs\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        k: v.outputs.output_data for k,v in jobs.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "pipeline = animal_images_preprocessing_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "# submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"image_preprocessing_pipeline\",\n",
    ")\n",
    "# open the pipeline in web browser\n",
    "webbrowser.open(pipeline_job.studio_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "data_split_component = command(\n",
    "    name=\"data_split\",\n",
    "    display_name=\"Data Splitting to Train and Test\",\n",
    "    description=\"Reads a data asset of images and combines them into a training and testing dataset\",\n",
    "    # We want to give the datasets as a dynamic input ...\n",
    "   inputs={\n",
    "        \"animal_1\": Input(type=\"uri_folder\"),\n",
    "        \"animal_2\": Input(type=\"uri_folder\"),\n",
    "        \"animal_3\": Input(type=\"uri_folder\"),\n",
    "        \"train_test_split_factor\": Input(type=\"number\"), # The percentage of the data to use as testing data, always a positive value\n",
    "    },\n",
    "    # ... and take the outputs as a dynamic output to override the training and testset locations.\n",
    "    outputs={\n",
    "        \"training_data\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        \"testing_data\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=os.path.join(\"components\", \"dataprep\"),\n",
    "    command=\"\"\"python traintestsplit.py \\\n",
    "            --datasets ${{inputs.animal_1}} ${{inputs.animal_2}} ${{inputs.animal_3}} \\\n",
    "            --training_data ${{outputs.training_data}} \\\n",
    "            --testing_data ${{outputs.testing_data}} \\\n",
    "            --split_size ${{inputs.train_test_split_factor}}\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    # environment=f\"aml-Pillow@latest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component data_split with Version 2023-10-10-12-24-12-9683420 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "data_split_component = ml_client.create_or_update(data_split_component.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {data_split_component.name} with Version {data_split_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=\"mlopsci\",\n",
    "    description=\"Custom data_prep pipeline\",\n",
    ")\n",
    "def animal_images_traintest_split_pipeline(\n",
    "    train_test_split: int, # Currently we don't use these version numbers, but we will use them later on.\n",
    "    animal_1: Input,\n",
    "    animal_2: Input,\n",
    "    animal_3: Input,\n",
    "):\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    # These are the animals with the version name as a second item in the tuple\n",
    "\n",
    "    # Combining arguments starting with \"animals_\" into a dictionary\n",
    "    animals_args = {k: v for k, v in locals().items() if k.startswith(\"animals_\")}\n",
    "\n",
    "    # Create a component instance by calling the component factory\n",
    "    data_split_job = data_split_component(\n",
    "            animal_1=animal_1,\n",
    "            animal_2=animal_2,\n",
    "            animal_3=animal_3,\n",
    "            train_test_split_factor=train_test_split\n",
    "        )\n",
    "    \n",
    "    # Override the training data output and testing data output to a file named \"trainingdata\" and \"testingdata\n",
    "    data_split_job.outputs.training_data = Output(\n",
    "        type=\"uri_folder\",\n",
    "        name=\"training_data\",\n",
    "        mode=\"rw_mount\"\n",
    "    )\n",
    "    data_split_job.outputs.testing_data = Output(\n",
    "        type=\"uri_folder\",\n",
    "        name=\"testing_data\",\n",
    "        mode=\"rw_mount\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # a pipeline returns a dictionary of outputs\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        \"training_data\": data_split_job.outputs.training_data,\n",
    "        \"testing_data\": data_split_job.outputs.testing_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'animal_1': {'type': 'uri_folder', 'path': 'azureml:pandas_resized:1'}, 'animal_2': {'type': 'uri_folder', 'path': 'azureml:cats_resized:1'}, 'animal_3': {'type': 'uri_folder', 'path': 'azureml:dogs_resized:1'}}\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "version = \"1\" # We can choose which version of the resized_pandas it will use\n",
    "animals = [\"pandas\", \"cats\", \"dogs\"]\n",
    "\n",
    "# Apparently, we made a small mistake in the naming conventions, but we will ignore that for now, we can fix it later...\n",
    "animals_datasets = {\n",
    "    f\"animal_{i+1}\": Input(type=\"uri_folder\", path=f\"azureml:{animal}_resized:{version}\")\n",
    "    for i, animal in enumerate(animals)\n",
    "}\n",
    "\n",
    "print(animals_datasets)\n",
    "\n",
    "train_test_pipeline = animal_images_traintest_split_pipeline(\n",
    "    **animals_datasets,\n",
    "    train_test_split=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit the pipeline job\n",
    "train_test_pipeline_job = ml_client.jobs.create_or_update(\n",
    "    train_test_pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"image_preprocessing_pipeline\",\n",
    ")\n",
    "# open the pipeline in web browser\n",
    "webbrowser.open(train_test_pipeline_job.studio_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name aml-Tensorflow-Pillow is registered to workspace, the environment version is 2\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "import os\n",
    "\n",
    "custom_env_name = \"aml-Tensorflow-Pillow\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for AI Training (with Pillow)\",\n",
    "    tags={\"Pillow\": \"0.0.1\", \"Tensorflow\": \"2.4.1\"},\n",
    "    conda_file=os.path.join(\"components\", \"training\", \"conda.yaml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "training_component = command(\n",
    "    name=\"training\",\n",
    "    display_name=\"Training an AI model\",\n",
    "    description=\"Trains an AI model by inputting a lot of training and testing data.\",\n",
    "    inputs={\n",
    "        \"training_folder\": Input(type=\"uri_folder\"),\n",
    "        \"testing_folder\": Input(type=\"uri_folder\"),\n",
    "        \"epochs\": Input(type=\"number\") # The percentage of the data to use as testing data, always a positive value\n",
    "    },\n",
    "    outputs={\n",
    "        \"output_folder\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=os.path.join(\"components\", \"training\"),\n",
    "    command=\"\"\"python train.py \\\n",
    "            --training_folder ${{inputs.training_folder}} \\\n",
    "            --testing_folder ${{inputs.testing_folder}} \\\n",
    "            --output_folder ${{outputs.output_folder}} \\\n",
    "            --epochs ${{inputs.epochs}} \\\n",
    "            \"\"\",\n",
    "    # environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    environment=f\"aml-Tensorflow-Pillow@latest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component training with Version 2023-10-10-12-16-00-3187527 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "training_component = ml_client.create_or_update(training_component.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {training_component.name} with Version {training_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=\"mlopsci\",\n",
    "    description=\"Custom Animals Training pipeline\",\n",
    ")\n",
    "def animals_training_pipeline(\n",
    "    training_folder: Input, # Currently we don't use these version numbers, but we will use them later on.\n",
    "    testing_folder: Input,\n",
    "    epochs: int,\n",
    "):\n",
    "\n",
    "    training_job = training_component(\n",
    "        training_folder=training_folder,\n",
    "        testing_folder=testing_folder,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    \n",
    "    # Let Azure decide a unique place everytime\n",
    "    training_job.outputs.output_folder = Output(\n",
    "        type=\"uri_folder\",\n",
    "        name=\"output_data\",\n",
    "        mode=\"rw_mount\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # a pipeline returns a dictionary of outputs\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        \"output_data\": training_job.outputs.output_folder,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "\n",
    "# Woops, make sure to use the correct version number here!\n",
    "training_pipeline = animals_training_pipeline(\n",
    "    # Change these versions if you want to override the choices\n",
    "    training_folder=Input(type=\"uri_folder\", path=f\"azureml:training_data:1\"),\n",
    "    testing_folder=Input(type=\"uri_folder\", path=f\"azureml:testing_data:1\"),\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "# submit the pipeline job\n",
    "training_pipeline_job = ml_client.jobs.create_or_update(\n",
    "    training_pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"training_pipeline\",\n",
    ")\n",
    "# open the pipeline in web browser\n",
    "webbrowser.open(training_pipeline_job.studio_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlopsVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
